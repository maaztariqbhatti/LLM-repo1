import os
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain, LLMRouterChain, MultiPromptChain, HypotheticalDocumentEmbedder, RetrievalQA
import dotenv
from langchain_core.prompts import PromptTemplate
from typing import Optional
import json
import pandas as pd
from Text_preprocessing import Text_preprocessing
from langchain_community.document_loaders import DataFrameLoader
from typing import List, Dict, Any, Mapping
from langchain.globals import set_debug

from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

from langchain_community.vectorstores import chroma as Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_core.vectorstores import VectorStoreRetriever
from pydantic.v1 import Field
from langchain_core.documents import Document
from langchain_community.document_transformers import (
    LongContextReorder
)
from sentence_transformers import CrossEncoder
import datetime
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    context_relevancy,
    answer_correctness,
    answer_similarity,
    context_entity_recall
)
import llmModels 
import wandb
import pickle
import prompts
import groundTruths


dotenv.load_dotenv()

### PARAMETER CHECKPOINT ####

#Open AI
chatModelAI = ChatOpenAI()

# # # # Llama 2 13 B chat
# chatModel_llama13b = llmModels.loadLlamma()

# # # Mistral 7B chat
# chatModel_mistral7b = llmModels.loadMistral7b()

# #70B
# chatModel_llama70b = llmModels.loadLlama2_70B()

## Llama 3 8B
chatModel_llama3_8B = llmModels.loadLlama3_8B()

##FSD_1777
dataPath = "/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json"
dateFrom = "2023-10-19T18:58:41+00:00" #2023-10-19T18:58:41Z for 200 tweets
dateTo = "2023-10-19 23:58:47+00:00"

##FSD_1555
# dataPath = "/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_21_59_22TO23_59_59.pkl"
# dateFrom = "2023-06-01 22:59:45+00:00" 
# dateTo = "2023-06-01 23:59:59+00:00" #200 tweets labelled

class CustomRetriever(VectorStoreRetriever):
    """Implements re ranking of retriever output using cross encoder"""
    vectorstore: VectorStoreRetriever
    search_type: str = "similarity"
    search_kwargs: dict = Field(default_factory=dict)

    def get_relevant_documents(self, query: str) -> List[Document]:
        docs = self.vectorstore.get_relevant_documents(query=query)

        # Cross encoder re ranking 
        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        pairs = []
        for doc in docs:
            pairs.append([query, doc.page_content])
        scores = cross_encoder.predict(pairs)

        i = 0
        #Add scores to document
        for doc in docs:
            doc.metadata["cross-encoder_score"] = scores[i]
            i = i+1
        #Sort documents according to score
        sorted_docs = sorted(docs, key=lambda doc: doc.metadata.get('cross-encoder_score'), reverse=True)

        # #Remove 10 worst performing docs from the list 
        sorted_docs = sorted_docs[:-10]

        #Re order according to long text re-order (Important context in start and end)
        reordering = LongContextReorder()
        reordered_docs = reordering.transform_documents(sorted_docs)

        #Remove cross encoder score so re ordered context is back to its orignal form
        for doc in reordered_docs:
            doc.metadata.pop("cross-encoder_score")
        return reordered_docs

"""Load relevant fields of flood tags api json response"""
def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):
    # Load json and extract relevant records in pandas df
    with open(dataPath, 'r') as json_file:
        response_dict = json.load(json_file)

    # Convert to pandas df    
    pd.set_option('display.max_colwidth', None)
    df = pd.DataFrame(response_dict)
    df['date'] = pd.to_datetime(df['date'])
    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])

    #Get data between thresholds
    threshold_datetime_lower = pd.to_datetime(dateFrom)
    threshold_datetime_upper = pd.to_datetime(dateTo)
    df = df[df['date'] >= threshold_datetime_lower]
    df = df[df['date'] <= threshold_datetime_upper]

    #Remove duplicates
    df  = df.drop_duplicates(subset=["text"], keep=False)
    #Pre-process
    preprocess = Text_preprocessing(df)
    df = preprocess.preprocess()
    #Covert date to string
    df['date'] = df['date'].astype(str)
    return df

def bgeEmbeddings():
    model_name = "BAAI/bge-large-en-v1.5"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity
    model = HuggingFaceBgeEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    return model

"""Load relevant fields of flood tags api json response"""
def dataframe_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):
    # Loading pandas dataframe from picke file
    with open(dataPath, 'rb') as f:
        data = pickle.load(f)

    df = pd.DataFrame(data)
    df['date'] = pd.to_datetime(df['date'])
    # df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])

    #Get data between thresholds
    threshold_datetime_lower = pd.to_datetime(dateFrom)
    threshold_datetime_upper = pd.to_datetime(dateTo)
    df = df[df['date'] >= threshold_datetime_lower]
    df = df[df['date'] <= threshold_datetime_upper]

    #Remove duplicates
    df  = df.drop_duplicates(subset=["text"], keep=False)
    
    #Covert date to string
    df['date'] = df['date'].astype(str)
    return df


def hydeEmbedder(embeddingsModel):
    
    model = chatModelAI
    prompt_template  = prompts.prompt_template_HyDE_OpenAI
    prompt = PromptTemplate(input_variables=["question"], template= prompt_template)
    llm_chain_hyde  = LLMChain(llm = model, prompt=prompt)

    embeddings = HypotheticalDocumentEmbedder(llm_chain=llm_chain_hyde,
                                                base_embeddings=embeddingsModel,
                                                verbose=True)
    return embeddings


def data_embedding(data : list, eModel = "bge-large-en-v1.5", rType = "Query"):
    """Vectorize the data using OpenAI embeddings and store in Chroma db"""
    if eModel != "bge-large-en-v1.5":
        embeddings = OpenAIEmbeddings()
    else:
        embeddings = bgeEmbeddings()
    
    if (rType == "Hyde"):
        embeddings = hydeEmbedder(embeddings)

    documents = []
    loader = DataFrameLoader(data, page_content_column="text")
    documents.extend(loader.load())

    #Change this -- removal of duplicates
    db = Chroma.Chroma.from_documents(documents,embeddings)
    if db._client.list_collections() != None:
        for collection in db._client.list_collections():
            ids = collection.get()['ids']
            print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))
            if len(ids): collection.delete(ids)

    #Create a vector store
    db = Chroma.Chroma.from_documents(documents,embeddings)
    return db

"""For running a single query"""
def predictions_response(question, eModel = "bge-large-en-v1.5", rType = "Query", rerank = True,k = 25):
    
    #  LLM initialisation
    model = chatModel_llama3_8B

    # Load the data from source
    data = json_dataloader()

    # Loading pandas dataframe from picke file
    # data = dataframe_dataloader()

    # Convert to vector store
    vectorstore = data_embedding(data, eModel= eModel, rType= rType)

        # Get retriever
    if rerank == True:
        retriever = CustomRetriever(vectorstore=vectorstore.as_retriever(search_kwargs={'k': k+5}))
    else:
        retriever = vectorstore.as_retriever(search_kwargs={'k': k})


    default_prompt = PromptTemplate(template = prompts.prompt_template_llama3_loc, input_variables = ['question', 'context'])
    default_chain = RetrievalQA.from_chain_type(llm = model,
                            chain_type='stuff',
                            retriever=retriever,
                            chain_type_kwargs={"prompt": default_prompt}
                            )
    
    default_chain.invoke(question)


def train_sweeps(config = None):

    with wandb.init(config=config):

        config = wandb.config
         
        # # Load the data from source
        data = json_dataloader()

        # Loading pandas dataframe from picke file
        # data = dataframe_dataloader()

        # Convert to vector store
        if (config.Retrieval_type == "HYDE"):
            vectorstore = data_embedding(data, rType="Hyde")
        else:
            vectorstore = data_embedding(data)

        # Get retriever
        if config.Cross_Encoder_rerank == "Yes":
            retriever = CustomRetriever(vectorstore=vectorstore.as_retriever(search_kwargs={'k': config.k+5}))
        else:
            retriever = vectorstore.as_retriever(search_kwargs={'k': config.k})

        #  LLM initialisation
        if (config.LLM == "Llama-2-13b-chat-hf"):
            model = chatModel_llama13b  
            default_prompt = PromptTemplate(template = prompts.prompt_template_llama_loc, input_variables = ['question', 'context'])

        if (config.LLM == "Mistral-7B-Instruct-v0.2"):
            model = chatModel_mistral7b
            default_prompt = PromptTemplate(template = prompts.prompt_template_mistral, input_variables = ['question', 'context'])

        # if (config.LLM == "Llama-2-70b-chat-hf"):
        #     model = chatModel_llama70b  
        #     default_prompt = PromptTemplate(template = prompts.prompt_template_llama_loc, input_variables = ['question', 'context'])



        #Open AI template
        # default_prompt_template = """Answer the question based only on the following tweet's context: {context}
        # Question: {question}"""

        
        default_chain = RetrievalQA.from_chain_type(llm = model,
                                chain_type='stuff',
                                retriever=retriever,
                                chain_type_kwargs={"prompt": default_prompt}
                                )
        
        #Evaluation using RAGAS
        questions = ["Which locations received a flood warning?"]


        #FSD_XXXX
        ground_truths = groundTruths.ground_truths_FSD1777
        

        answers = []
        contexts = []

        # Inference 
        for query in questions:
            answers.append(default_chain.invoke(query)['result'])
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

        # To dict
        data = {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truths": ground_truths
        }

        # Convert dict to dataset
        dataset = Dataset.from_dict(data)

        result = evaluate(
        dataset = dataset, 
        metrics=[
        answer_correctness,
        context_entity_recall,
        ],
        )

        RAG_response = wandb.Table(columns=["Query", "Response", "Context"], data=[[questions, answers, contexts]])

        wandb.log({"answer_correctness": result['answer_correctness'],
                    "context_entity_recall": result['context_entity_recall'],
                    "retriever": vectorstore._collection.get()['ids'],
                    "RAG response": RAG_response
                   })
        

"""Run experiments"""
def run_sweep():
        # # 1. Sweep configurations
    sweep_config = {
        'method': 'grid'
    }

    parameters_dict = {
    'k': {
        'values': [50]
        },
    'LLM': {
        'values': ["Llama-2-13b-chat-hf", "Mistral-7B-Instruct-v0.2"]
        },
    'Cross_Encoder_rerank': {
        'values' : ["No"]
        },
    'Retrieval_type': {
        'values' : ["Query"]
    }
    }

    # parameters_dict = {
    # 'k': {
    #     'values': [55]
    #     },
    # 'LLM': {
    #     'values': ["Mistral-7B-Instruct-v0.2", "Mistral-7B-Instruct-v0.2"] #"Mistral-7B-Instruct-v0.2"
    #     },
    # 'Cross_Encoder_rerank': {
    #     'values' : ["No","Yes"]
    #     },
    # 'Retrieval_type': {
    #     'values' : ["Query"]
    # }
    # }

    # parameters_dict = {
    # 'k': {
    #     'values': [25]
    #     },
    # 'LLM': {
    #     'values': ["Llama-2-13b-chat-hf"] #"Mistral-7B-Instruct-v0.2"
    #     },
    # 'Cross_Encoder_rerank': {
    #     'values' : ["Yes"]
    #     },
    # 'Retrieval_type': {
    #     'values' : ["Query"]
    # }
    # }

    sweep_config['parameters'] = parameters_dict    

    #Initialise sweep
    sweep_id = wandb.sweep(sweep_config, project="FW_1777_AER_LongContext_LLMs")

    wandb.agent(sweep_id, train_sweeps)

if __name__ == "__main__":

    prompt = "Which places are specifically receving evacutaion orders?"
    predictions_response(prompt)
    # run_sweep()





                                                    














