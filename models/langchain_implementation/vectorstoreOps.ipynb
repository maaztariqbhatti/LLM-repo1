{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbhatti/miniconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_community.vectorstores import chroma as Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from Text_preprocessing import Text_preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions \n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FSD_1555\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_Japan_06_15.pkl\"\n",
    "# dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "#Remove duplicates\n",
    "# df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>2023-06-01 12:59:40+00:00</td>\n",
       "      <td>美濃加茂は川あり山あり、平地あり。 自分の子供の頃は、太田地区の水害は毎年の事でしたが。 今は、だいぶ整理されたとは言え、局地的に降るとどうなるか読めませんよね。 大事に至らないのを祈るばかりです</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>2023-06-01 12:58:00+00:00</td>\n",
       "      <td>線状降水帯が発生した場合は、局地的に雨量が増えるおそれがあります。 西日本から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>2023-06-01 12:57:26+00:00</td>\n",
       "      <td>◉台風2号接近のため注意喚起 気象庁の予報によると、明日6/2-6/3は台風接近に伴い、雨風が強まる可能性があります。明日、横浜は開港記念日。大変残念ですが安全が第一。都筑区の浸水ハザードマップ。川沿いエリアは特に注意してください。 #横浜市 の他エリアはこちらから</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>2023-06-01 12:57:16+00:00</td>\n",
       "      <td>【あす2日　「災害級の大雨」の恐れ　線状降水帯発生の可能性も　避難の心構えを】tenkijp とは言うが台風の勢力は衰え始め、東京は関西程ではないのでは？と希望的観測。 全ては梅雨前線次第</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>2023-06-01 12:57:02+00:00</td>\n",
       "      <td>あれよな3.11ぐらいまではさ　ネタ無くて ニュースも　みーーーーーーんな 『藤井名人誕生』みたいな!!そう言うネタばかりだった 思うと3.11以降・・色々起き過ぎたわな;;思えばいがみ合いもここ等辺から原発・燃料そして熊本に洪水とコロナとロシアと・・そろそろ平穏期入って欲しいな。 昔はさ・・・</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>2023-06-01 06:09:02+00:00</td>\n",
       "      <td>台風２号の影響で3日にかけて東海3県は大雨の恐れ　土砂災害や浸水に警戒(メ〜テレ（名古屋テレビ）) #Yahooニュース</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>2023-06-01 06:08:47+00:00</td>\n",
       "      <td>まさかの2か月連続台風… それが無ければフローラ姫さんか、あやママさんで決まってたかな？と、思うけど、それも桃鉄。 　 でも、珍しいも。 水俣の畑に被害は無く… ほっとしました。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>2023-06-01 06:05:16+00:00</td>\n",
       "      <td>ハザードマップを確認 台風2号の北上にともなって、 日本の南にある前線の活動が活発になる見込みで、 東海3県では、今夜から3日にかけて雷をともなった非常に激しい雨が降るところがある見込みです。 自分が住む地域の災害の危険性を事前に確認し、避難の場所や方法を確認しておきましょう。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>2023-06-01 06:01:47+00:00</td>\n",
       "      <td>今後36時間で、300mmを超えるような予想のところもあります。この図はあくまでコンピュータの予想で、局地的にはそれよりも多くなる可能性もあります。 南西諸島から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>2023-06-01 06:00:16+00:00</td>\n",
       "      <td>本日夜から3日（土曜）明け方にかけて、大気の状態が非常に不安定となり、堺市においても激しい雨が降るおそれがあります。河川氾濫や土砂災害、道路の冠水などに注意が必要です。いざという時に慌てず行動できるよう、事前の備えを確認しておきましょう。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  \\\n",
       "6817  2023-06-01 12:59:40+00:00   \n",
       "6818  2023-06-01 12:58:00+00:00   \n",
       "6819  2023-06-01 12:57:26+00:00   \n",
       "6820  2023-06-01 12:57:16+00:00   \n",
       "6821  2023-06-01 12:57:02+00:00   \n",
       "...                         ...   \n",
       "7372  2023-06-01 06:09:02+00:00   \n",
       "7373  2023-06-01 06:08:47+00:00   \n",
       "7374  2023-06-01 06:05:16+00:00   \n",
       "7375  2023-06-01 06:01:47+00:00   \n",
       "7376  2023-06-01 06:00:16+00:00   \n",
       "\n",
       "                                                                                                                                                      text  \n",
       "6817                                                    美濃加茂は川あり山あり、平地あり。 自分の子供の頃は、太田地区の水害は毎年の事でしたが。 今は、だいぶ整理されたとは言え、局地的に降るとどうなるか読めませんよね。 大事に至らないのを祈るばかりです  \n",
       "6818                                                                      線状降水帯が発生した場合は、局地的に雨量が増えるおそれがあります。 西日本から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。  \n",
       "6819                 ◉台風2号接近のため注意喚起 気象庁の予報によると、明日6/2-6/3は台風接近に伴い、雨風が強まる可能性があります。明日、横浜は開港記念日。大変残念ですが安全が第一。都筑区の浸水ハザードマップ。川沿いエリアは特に注意してください。 #横浜市 の他エリアはこちらから  \n",
       "6820                                                        【あす2日　「災害級の大雨」の恐れ　線状降水帯発生の可能性も　避難の心構えを】tenkijp とは言うが台風の勢力は衰え始め、東京は関西程ではないのでは？と希望的観測。 全ては梅雨前線次第  \n",
       "6821  あれよな3.11ぐらいまではさ　ネタ無くて ニュースも　みーーーーーーんな 『藤井名人誕生』みたいな!!そう言うネタばかりだった 思うと3.11以降・・色々起き過ぎたわな;;思えばいがみ合いもここ等辺から原発・燃料そして熊本に洪水とコロナとロシアと・・そろそろ平穏期入って欲しいな。 昔はさ・・・  \n",
       "...                                                                                                                                                    ...  \n",
       "7372                                                                                          台風２号の影響で3日にかけて東海3県は大雨の恐れ　土砂災害や浸水に警戒(メ〜テレ（名古屋テレビ）) #Yahooニュース  \n",
       "7373                                                             まさかの2か月連続台風… それが無ければフローラ姫さんか、あやママさんで決まってたかな？と、思うけど、それも桃鉄。 　 でも、珍しいも。 水俣の畑に被害は無く… ほっとしました。  \n",
       "7374           ハザードマップを確認 台風2号の北上にともなって、 日本の南にある前線の活動が活発になる見込みで、 東海3県では、今夜から3日にかけて雷をともなった非常に激しい雨が降るところがある見込みです。 自分が住む地域の災害の危険性を事前に確認し、避難の場所や方法を確認しておきましょう。  \n",
       "7375                            今後36時間で、300mmを超えるような予想のところもあります。この図はあくまでコンピュータの予想で、局地的にはそれよりも多くなる可能性もあります。 南西諸島から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。  \n",
       "7376                               本日夜から3日（土曜）明け方にかけて、大気の状態が非常に不安定となり、堺市においても激しい雨が降るおそれがあります。河川氾濫や土砂災害、道路の冠水などに注意が必要です。いざという時に慌てず行動できるよう、事前の備えを確認しておきましょう。  \n",
       "\n",
       "[560 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FSD_1777\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json\"\n",
    "dateFrom = \"2023-10-19T09:00:00+00:00\" #2023-10-19T18:58:41Z for 200 tweets\n",
    "dateTo = \"2023-10-19T18:00:00+00:00\"\n",
    "\n",
    "\"\"\"Load relevant fields of flood tags api json response\"\"\"\n",
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=True)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from FlagEmbedding import FlagReranker\n",
    "from pydantic.v1 import Field\n",
    "from typing import List, Dict, Any, Mapping\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents import Document\n",
    "#Cross encoder re rank\n",
    "class Custom_ja_Retriever(VectorStoreRetriever):\n",
    "    \"\"\"Implements re ranking of retriever output using cross encoder\"\"\"\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    search_type: str = \"similarity\"\n",
    "    search_kwargs: dict = Field(default_factory=dict)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"Extract relevant records using japanese embedding model\"\"\"\n",
    "\n",
    "        #Translate query to japanese \n",
    "        # query_ja = llmModels.bulk_translation(query, source_lan = \"en_XX\", output_lan= \"ja_XX\")\n",
    "        # query = query_ja[0]\n",
    "\n",
    "        #TRANSLATION -------------------------------------------------------------------\n",
    "        #Retrieve japanese records\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "\n",
    "        dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "        dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "        dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "        # Loading pandas dataframe from picke file\n",
    "        with open(dataPath, 'rb') as f:\n",
    "            data_eng = pickle.load(f)\n",
    "\n",
    "        df = pd.DataFrame(data_eng)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        # df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "        #Get data between thresholds\n",
    "        threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "        threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "        df = df[df['date'] >= threshold_datetime_lower]\n",
    "        df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "        #Remove duplicates\n",
    "        # df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "        #Covert date to string\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        data_eng = df\n",
    "\n",
    "        #Store japanese tweets in an array \n",
    "        ja_tweets = []\n",
    "        for doc in docs:\n",
    "            ja_tweets.append(doc.page_content)\n",
    "\n",
    "        #Get index of these extracted tweets\n",
    "        indexes = []\n",
    "        for tweet in ja_tweets:\n",
    "            indexes.append(data[data['text'] ==tweet].index[0])\n",
    "\n",
    "                #Extract english from exact location\n",
    "        docs_en = data_eng.loc[indexes].text.to_list()\n",
    "\n",
    "        #Chnage text to english \n",
    "        ind = 0\n",
    "        for doc in docs:\n",
    "            doc.page_content = docs_en[ind]\n",
    "            ind = ind+1\n",
    "\n",
    "\n",
    "        # Cross encoder re ranking ------------------------------------------\n",
    "        #For japanese query translation\n",
    "        query = \"Which areas are subject to flood warnings due to heavy rain?\"\n",
    "        cross_encoder = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "        pairs = []\n",
    "        for doc in docs:\n",
    "            pairs.append([query, doc.page_content])\n",
    "        scores = cross_encoder.compute_score(pairs)\n",
    "\n",
    "        i = 0\n",
    "        #Add scores to document\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"cross-encoder_score\"] = scores[i]\n",
    "            i = i+1\n",
    "        #Sort documents according to score\n",
    "        sorted_docs = sorted(docs, key=lambda doc: doc.metadata.get('cross-encoder_score'), reverse=True)\n",
    "\n",
    "        #Remove 20 worst performing docs from the list \n",
    "        sorted_docs = sorted_docs[:k_cross]\n",
    "\n",
    "        # #Re order according to long text re-order (Important context in start and end)\n",
    "        # reordering = LongContextReorder()\n",
    "        # reordered_docs = reordering.transform_documents(sorted_docs)\n",
    "\n",
    "        #Remove cross encoder score so re ordered context is back to its orignal form\n",
    "        for doc in sorted_docs:\n",
    "            doc.metadata.pop(\"cross-encoder_score\")\n",
    "\n",
    "        #Change updated docs back to docs\n",
    "        docs = sorted_docs\n",
    "\n",
    "        ##Extract japanese tweet texts -----------\n",
    "        # tweets_JA = []\n",
    "        # for doc in docs:\n",
    "        #     tweets_JA.append(doc.page_content)\n",
    "\n",
    "        # #Translate text feild to english\n",
    "        # # docs_en = llmModels.bulk_translation(tweets_JA)\n",
    "\n",
    "        # #Chnage text to english \n",
    "        # ind = 0\n",
    "        # for doc in docs:\n",
    "        #     doc.page_content = docs_en[ind]\n",
    "        #     ind = ind+1\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgeEmbeddings():\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    # model_name = \"BAAI/bge-m3\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVE 555 document(s) from langchain collection\n"
     ]
    }
   ],
   "source": [
    "embeddings = bgeEmbeddings()\n",
    "\n",
    "# embeddings = SentenceTransformer('pkshatech/GLuCoSE-base-ja')\n",
    "# embeddings = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"pkshatech/GLuCoSE-base-ja\")\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"pkshatech/GLuCoSE-base-ja\")\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "#Create a vector store\n",
    "# db = Chroma.Chroma(\"Langchain collection\",embeddings)\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings)\n",
    "if db._client.list_collections() != None:\n",
    "\n",
    "  for collection in db._client.list_collections():\n",
    "    ids = collection.get()['ids']\n",
    "    print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "    if len(ids): collection.delete(ids)\n",
    "\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all embeddings\n",
    "len(db._collection.get()['ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k': 50})\n",
    "query = \"\"\"Act as a location extrator and extract all the relevant locations receiving flood warnings\"\"\"\n",
    "# k_cross = 50\n",
    "# retriever = Custom_ja_Retriever(vectorstore=db.as_retriever(search_kwargs={'k': 100}))\n",
    "\n",
    "# query = \"\"\"Which locations are likely to receive flood warnings due to heavy rain?\"\"\"\n",
    "docs = retriever.get_relevant_documents(query=query)\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"The Red warning's been extended to: Aberdeenshire Angus Dundee Perth &amp; Kinross This means extremely dangerous travel conditions and floodwater could cause a danger to life. Please avoid travel in these areas. More safety advice here:\", metadata={'date': '2023-10-19 13:23:57+00:00'}), Document(page_content=\"Weather warnings across the UK. If you're affected by flooding over the next few days, ServiceMaster Restore are on call 24/7 to sort the incident quickly, and minimise damage to your home or business. #weatherwarnings #ServiceMasterRestore\", metadata={'date': '2023-10-19 10:45:03+00:00'}), Document(page_content='The Amber alert for rain has been extended to several new areas across England, Friday – Saturday. Our risk management team have provided guidance on preparing for floods and we recommend checking for updates in your area.', metadata={'date': '2023-10-19 10:34:15+00:00'}), Document(page_content='The Amber alert for rain has been extended to several new areas across England, Friday – Saturday. Our risk management team have provided guidance on preparing for floods and we recommend checking for updates in your area.', metadata={'date': '2023-10-19 10:38:12+00:00'}), Document(page_content=\"Possible evacuations as red warning extended Residents of Brechin may need to be evacuated if the Angus town's flood defences are at risk of being breached, Scotland's Environmental Protection Agency says. #StormBabet\", metadata={'date': '2023-10-19 11:15:02+00:00'}), Document(page_content='Storm Babet triggers a red alert in the UK, with exceptional rainfall and flooding posing a “danger to life\"', metadata={'date': '2023-10-19 15:23:35+00:00'}), Document(page_content='Storm Babet triggers a red alert in the UK, with exceptional rainfall and flooding posing a “danger to life\"', metadata={'date': '2023-10-19 15:40:13+00:00'}), Document(page_content=\"Storm Babet news LIVE: Met Office EXPANDS red weather warning as floods spark chaos with 'unprecedented' weather heading for UK\", metadata={'date': '2023-10-19 11:07:47+00:00'}), Document(page_content='The Amber alert for rain has been extended to several new areas across England, Friday – Saturday. See our risk management guidance on preparing for floods, stay safe and check for updates in your area.', metadata={'date': '2023-10-19 10:42:37+00:00'}), Document(page_content='Angus Glens region in Scotland. The red warning hasn’t come into force yet and already we’re seeing scenes like this. Potential for historic levels of flooding. Awful.', metadata={'date': '2023-10-19 16:19:56+00:00'}), Document(page_content='**FLOOD WARNING** at Inverurie', metadata={'date': '2023-10-19 17:34:15+00:00'}), Document(page_content='**FLOOD WARNING** at North Sea at North and South Shields (North Tyneside, South Tyneside)', metadata={'date': '2023-10-19 16:41:14+00:00'}), Document(page_content='stay safe up there please UK weather: Storm Babet forces evacuations in red alert area of Brechin due to floods', metadata={'date': '2023-10-19 12:52:38+00:00'}), Document(page_content='Update: Five places in England have been given flood warnings: Barbourne in Worcester; the River Maun near Retford; the coastlines at Bridlington and Scarborough in Yorkshire; and the banks of the Tyne at North and South Shields in Newcastle.', metadata={'date': '2023-10-19 13:07:12+00:00'}), Document(page_content='Storm Babet - WARNINGS UPDATE The Met Office have extended the area of the RED warning for HEAVY RAIN in eastern Scotland. An AMBER warning for HEAVY RAIN has also been issued for parts of N England, the Midlands and Borders All the warning details:', metadata={'date': '2023-10-19 11:21:17+00:00'}), Document(page_content='Breaking: All residents have been told to leave the town of Brechin in Angus, Scotland. A severe flood warning is set to be put in place for the Brechin River and South Esk area, Angus Council has said. Those in the affected areas should leave their homes', metadata={'date': '2023-10-19 11:32:34+00:00'}), Document(page_content='Evacuations have just been ordered for the town of Brechin, Angus, alongside a severe flood warning . #Brechin #Angus #Scotlandweather #floodrisk #flooding', metadata={'date': '2023-10-19 11:52:03+00:00'}), Document(page_content='UK weather: Storm Babet forces evacuations in red alert area of Angus due to floods', metadata={'date': '2023-10-19 17:45:02+00:00'}), Document(page_content='Beginning to see reports of roads around Glenrothes being flooded and at least one blocked by a fallen tree. We may be just outside the full Red Warning area but an Amber Warning is still serious. Please think very carefully about whether your journey is necessary.', metadata={'date': '2023-10-19 12:09:03+00:00'}), Document(page_content='Mandatory evacuations have just been issued for the town of Brechin, Angus, alongside a severe flood warning . #Brechin #Angus #Scotlandweather #floodrisk #flooding', metadata={'date': '2023-10-19 11:43:43+00:00'}), Document(page_content='Storm Babet: Red weather warning extended to Perth and Dundee as preparations for damage control underway | The bout of extreme weather is expected to bring severe flooding and disruption', metadata={'date': '2023-10-19 11:12:46+00:00'}), Document(page_content='The Met Office is warning parts of the UK could be cut off by flooding as Storm Babet batters the country. A new amber warning for rain has been issued for parts of northern England, the Midlands and Wales, and a rare red weather warning is in place in Scotland.', metadata={'date': '2023-10-19 14:00:00+00:00'}), Document(page_content='Warnings increased as Storm Babet batters England Homes and businesses are likely to be flooded, and deep floodwaters could cause danger to life. #... #Babet #batters #England #increased #Storm #warnings', metadata={'date': '2023-10-19 12:29:39+00:00'}), Document(page_content='Flood warning update Red flood warnings have now been extended to include Perthshire as well as Aberdeenshire and Angus. Amber and yellow warnings are also in place across Scotland and the National Park. For more flood updates visit #StormBabet', metadata={'date': '2023-10-19 17:29:33+00:00'}), Document(page_content='A Severe Flood Warning has been issued for Brechin. Follow for travel advice Follow for local advice Visit for more information on the Severe Flood Warning and the potential impacts.', metadata={'date': '2023-10-19 13:43:21+00:00'}), Document(page_content=\"Heavy rain and wind could create challenging travel conditions later in some parts of Wales Take care if you're out and about and stay up to date on the latest flooding information by calling Floodline for free on 0345 988 1188 or on our website\", metadata={'date': '2023-10-19 14:01:25+00:00'}), Document(page_content=\"The Met Office has issued red warnings for parts of the UK for severe weather due to #StormBabet Read our top tips to help protect your car from flood damage And here's what to do if your home's been flooded Stay safe!\", metadata={'date': '2023-10-19 14:11:46+00:00'}), Document(page_content='With weather warnings in force across Scotland, we want to encourage you to follow for detailed flooding updates', metadata={'date': '2023-10-19 15:03:00+00:00'}), Document(page_content='flooding in Forfar Angus getting pretty bad with red warning due to storm babet', metadata={'date': '2023-10-19 17:33:40+00:00'}), Document(page_content='Our officers are out clearing debris screens to reduce flood risk during Storm Babet - including these in Frodsham. Sign up for free flood warnings here: #TeamEA #Flood #StormBabet #PrepareActSurvive', metadata={'date': '2023-10-19 14:00:20+00:00'}), Document(page_content='Exceptionally red warning for heavy and persistent rain across eastern Scotland, some locations with 200 mm / 48h and potential flooding #StormBabet #WeatherAlert', metadata={'date': '2023-10-19 10:23:19+00:00'}), Document(page_content='Exceptionally red warning for heavy and persistent rain across eastern Scotland, some locations with 200 mm / 48h and potential flooding #StormBabet #WeatherAlert', metadata={'date': '2023-10-19 12:54:27+00:00'}), Document(page_content='Storm Babet brings risk to life with yellow, amber and red alerts throughout Scotland. Follow for updates and for flooding information. Avoid unnecessary travel and follow any advice from the emergency services.', metadata={'date': '2023-10-19 10:07:00+00:00'}), Document(page_content='UK weather: Storm Babet forces evacuations in red alert area of Brechin due to floods', metadata={'date': '2023-10-19 14:03:42+00:00'}), Document(page_content='UK weather: Storm Babet forces evacuations in red alert area of Brechin due to floods', metadata={'date': '2023-10-19 11:48:42+00:00'}), Document(page_content='Our Water &amp; Flood Rescue Technicians are monitoring the threat of severe flooding for parts of Scotland from #StormBabet. As an audited national asset, we are ready to assist if called upon.', metadata={'date': '2023-10-19 13:12:31+00:00'}), Document(page_content='Localised Flood Warning issued for Findhorn, Nairn, Moray and Speyside region', metadata={'date': '2023-10-19 12:47:34+00:00'}), Document(page_content='Storm Babet is coming, here is Martin Christmas, Operations Manager for West Yorkshire, explaining how we are preparing in Yorkshire. Make sure to sign up below for flood warnings:', metadata={'date': '2023-10-19 10:28:56+00:00'}), Document(page_content='The Met Office has issued an amber weather warning for rain across parts of North-East Wales from Friday at 12:00 BST, which is \"likely\" to cause flooding', metadata={'date': '2023-10-19 10:09:48+00:00'}), Document(page_content='Brechin has been evacuated due to a Danger to life flooding alert', metadata={'date': '2023-10-19 13:46:07+00:00'}), Document(page_content='Over 300 homes in #Brechin, #Scotland, are being evacuated due to a rare red weather warning caused by #StormBabet. The region anticipates severe #flooding and life-threatening conditions. Red warnings are issued only when there is a high likelihood of dangerous weather that…', metadata={'date': '2023-10-19 13:05:40+00:00'}), Document(page_content='Rainfall triggers dozens of landslides in Scotland. Submit outline ‘urgency’ grant. Asked if the process taking until min end Nov is an issue. Say no as roads ppl will get some data asap from a couple of the landslides to quant what we may miss. Told ok, no longer eligible', metadata={'date': '2023-10-19 14:55:09+00:00'}), Document(page_content='Storm Babet: Red weather warning as whole town evacuated - latest #ClimateActionNow #ClimateEmergency #ClimateChange #Flood #Scotland #GlobalBoiling #Babet', metadata={'date': '2023-10-19 14:01:57+00:00'}), Document(page_content='A rare RED rain warning is now in force Exceptional rainfall will fall across parts of eastern Scotland Severe flooding and disruption is expected with dangerous driving conditions Latest warning info', metadata={'date': '2023-10-19 17:12:24+00:00'}), Document(page_content='Urgent travel warning issued as Storm Babet brings heavy rain and flooding alert to Greater Manchester', metadata={'date': '2023-10-19 17:30:46+00:00'}), Document(page_content='UK weather: Storm #Babet forces #evacuations in red #alert area of #Brechin due to #floods', metadata={'date': '2023-10-19 15:27:16+00:00'}), Document(page_content='Storm Babet Live Flood Updates As Amber Weather Warning In Place For Derbyshire', metadata={'date': '2023-10-19 17:32:10+00:00'}), Document(page_content='Ireland and UK may have months worth of rain news has said from Storm babet few areas red level warnings of flooding', metadata={'date': '2023-10-19 12:15:03+00:00'}), Document(page_content='SEPA have issued a SEVERE FLOOD WARNING for Brechin. \"There is a possibility that the defences may be overtopped on Thursday late evening. Take action now. Stay away from flood water and do not take unnecessary risks\" Details:', metadata={'date': '2023-10-19 15:46:57+00:00'}), Document(page_content='BREAKING: Storm Babet forces evacuations in red alert area of Brechin due to floods Read more', metadata={'date': '2023-10-19 11:32:39+00:00'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m ja_tweets:\n\u001b[0;32m---> 35\u001b[0m     indexes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mtweet\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#Search for indexes over the english data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(indexes))\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/pandas/core/indexes/base.py:5366\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[1;32m   5364\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[1;32m   5365\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[0;32m-> 5366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   5369\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[1;32m   5370\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m   5371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "#FSD_1555 --- To track English tweets wothout translation \n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data_eng = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data_eng)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "#Remove duplicates\n",
    "# df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "data_eng = df\n",
    "\n",
    "#Store japanese tweets in an array \n",
    "ja_tweets = []\n",
    "for doc in docs:\n",
    "    ja_tweets.append(doc.page_content)\n",
    "\n",
    "#Get index of these extracted tweets\n",
    "indexes = []\n",
    "for tweet in ja_tweets:\n",
    "    indexes.append(data[data['text'] ==tweet].index[0])\n",
    "\n",
    "#Search for indexes over the english data\n",
    "print(len(indexes))\n",
    "print(data_eng.loc[indexes].text.to_list())\n",
    "# with open(\"Output.txt\", \"w\") as text_file:\n",
    "#     text_file.write(data_eng.loc[indexes].text.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_tweets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_string = ['String 1', 'String 2', 'String 3', 'String 4', 'String 5']\n",
    "\n",
    "#Extract japanese tweet texts\n",
    "tweets_JA = []\n",
    "ind = 0\n",
    "for doc in docs:\n",
    "    doc.page_content = eng_string[ind]\n",
    "    ind = ind+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing UAE embeddings\n",
    "from scipy import spatial\n",
    "input = \"Which locations have recieved an evacuation order?\"\n",
    "query = embeddings.embed_query (input)\n",
    "docs = retriever.get_relevant_documents(query=input)\n",
    "\n",
    "doc_vecs = []\n",
    "for doc in docs:\n",
    "    doc_vecs.append(embeddings.embed_query (doc.page_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = []\n",
    "for dv in doc_vecs:\n",
    "    cosine_sim.append(spatial.distance.cosine(query, dv))\n",
    "\n",
    "    # print(len(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Aberdeenshire\"\"\"\n",
    "# [d[1] for d in db.similarity_search_with_score(query, k=300)]\n",
    "outp =  db.similarity_search_with_score(query, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for element in cosine_sim:\n",
    "    if element < 0.58:\n",
    "        print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New embeddging model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "input_texts = [\n",
    "    \"what is the capital of China?\",\n",
    "    \"how to implement quick sort in python?\",\n",
    "    \"Beijing\",\n",
    "    \"sorting algorithms\"\n",
    "]\n",
    "\n",
    "model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = outputs.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(t).tolist() for t in texts]\n",
    "\n",
    "\n",
    "embeddings = MyEmbeddings()\n",
    "\n",
    "# splitter = SemanticChunker(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom embeddings with non-default embedding model\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        ##\n",
    "        #New embeddging model\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "        model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        batch_dict = tokenizer(input, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]\n",
    "                \n",
    "        return embeddings.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "custom_embeddings=MyEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "print(documents)\n",
    "\n",
    "# db = Chroma.Chroma.from_documents(documents,\n",
    "#                                   custom_embeddings,\n",
    "#                                   collection_metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
