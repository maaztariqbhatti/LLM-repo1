{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_community.vectorstores import chroma as Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from Text_preprocessing import Text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FSD_1555\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_21_59_22TO23_59_59.pkl\"\n",
    "dateFrom = \"2023-06-01 22:59:45+00:00\" \n",
    "dateTo = \"2023-06-01 23:59:59+00:00\" #200 tweets labelled\n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FSD_1777\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json\"\n",
    "dateFrom = \"2023-10-19T09:00:00+00:00\" #2023-10-19T18:58:41Z for 200 tweets\n",
    "dateTo = \"2023-10-19T18:00:00+00:00\"\n",
    "\n",
    "\"\"\"Load relevant fields of flood tags api json response\"\"\"\n",
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgeEmbeddings():\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbhatti/miniconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVE 555 document(s) from langchain collection\n"
     ]
    }
   ],
   "source": [
    "embeddings = bgeEmbeddings()\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "#Create a vector store\n",
    "# db = Chroma.Chroma(\"Langchain collection\",embeddings)\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings,\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "if db._client.list_collections() != None:\n",
    "\n",
    "  for collection in db._client.list_collections():\n",
    "    ids = collection.get()['ids']\n",
    "    print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "    if len(ids): collection.delete(ids)\n",
    "\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings,\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all embeddings\n",
    "len(db._collection.get()['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Which locations have been evacuated and which locations are getting evacutaion orders?\"\"\"\n",
    "docs = retriever.get_relevant_documents(query=query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing UAE embeddings\n",
    "from scipy import spatial\n",
    "input = \"Which locations have recieved an evacuation order?\"\n",
    "query = embeddings.embed_query (input)\n",
    "docs = retriever.get_relevant_documents(query=input)\n",
    "\n",
    "doc_vecs = []\n",
    "for doc in docs:\n",
    "    doc_vecs.append(embeddings.embed_query (doc.page_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = []\n",
    "for dv in doc_vecs:\n",
    "    cosine_sim.append(spatial.distance.cosine(query, dv))\n",
    "\n",
    "    # print(len(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Aberdeenshire\"\"\"\n",
    "# [d[1] for d in db.similarity_search_with_score(query, k=300)]\n",
    "outp =  db.similarity_search_with_score(query, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for element in cosine_sim:\n",
    "    if element < 0.58:\n",
    "        print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34229330651467094, 0.3523527454933534, 0.370738919434058, 0.3409977837027661, 0.3808665840456119, 0.37973994344030293, 0.37730797238417757, 0.35122532465563394, 0.35122532465563394, 0.3729195770924556, 0.36091296423524044, 0.3950091916938203, 0.3859390293303776, 0.36490902204834186, 0.39579932435002785, 0.3863409525292969, 0.35497307444649684, 0.40484377180522346, 0.39177352713349833, 0.380038328370378, 0.380038328370378, 0.3817885659419147, 0.40081366993277057, 0.40081366993277057, 0.40081366993277057, 0.39835584357576426, 0.4116979428866232, 0.4033720395728032, 0.4146771739478383, 0.3868499953945115, 0.39574133157486924, 0.38353882442642395, 0.42065178207880416, 0.41864865979655863, 0.41864865979655863, 0.3945974962308513, 0.39757451131286503, 0.39757451131286503, 0.4041586242397557, 0.36429657053695186, 0.4197360858733562, 0.4197360858733562, 0.4197360858733562, 0.4194068209661198, 0.42830807349480526, 0.42830807349480526, 0.3977146733165594, 0.3871695418771349, 0.3813732739941934, 0.4073258430145349, 0.41711782267561137]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New embeddging model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "input_texts = [\n",
    "    \"what is the capital of China?\",\n",
    "    \"how to implement quick sort in python?\",\n",
    "    \"Beijing\",\n",
    "    \"sorting algorithms\"\n",
    "]\n",
    "\n",
    "model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = outputs.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(t).tolist() for t in texts]\n",
    "\n",
    "\n",
    "embeddings = MyEmbeddings()\n",
    "\n",
    "# splitter = SemanticChunker(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom embeddings with non-default embedding model\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        ##\n",
    "        #New embeddging model\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "        model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        batch_dict = tokenizer(input, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]\n",
    "                \n",
    "        return embeddings.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "custom_embeddings=MyEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "print(documents)\n",
    "\n",
    "# db = Chroma.Chroma.from_documents(documents,\n",
    "#                                   custom_embeddings,\n",
    "#                                   collection_metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
