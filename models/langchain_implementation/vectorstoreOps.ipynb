{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbhatti/miniconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_community.vectorstores import chroma as Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from Text_preprocessing import Text_preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions \n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FSD_1555\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_Japan_06_15.pkl\"\n",
    "# dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "#Remove duplicates\n",
    "# df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>2023-06-01 12:59:40+00:00</td>\n",
       "      <td>美濃加茂は川あり山あり、平地あり。 自分の子供の頃は、太田地区の水害は毎年の事でしたが。 今は、だいぶ整理されたとは言え、局地的に降るとどうなるか読めませんよね。 大事に至らないのを祈るばかりです</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>2023-06-01 12:58:00+00:00</td>\n",
       "      <td>線状降水帯が発生した場合は、局地的に雨量が増えるおそれがあります。 西日本から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>2023-06-01 12:57:26+00:00</td>\n",
       "      <td>◉台風2号接近のため注意喚起 気象庁の予報によると、明日6/2-6/3は台風接近に伴い、雨風が強まる可能性があります。明日、横浜は開港記念日。大変残念ですが安全が第一。都筑区の浸水ハザードマップ。川沿いエリアは特に注意してください。 #横浜市 の他エリアはこちらから</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>2023-06-01 12:57:16+00:00</td>\n",
       "      <td>【あす2日　「災害級の大雨」の恐れ　線状降水帯発生の可能性も　避難の心構えを】tenkijp とは言うが台風の勢力は衰え始め、東京は関西程ではないのでは？と希望的観測。 全ては梅雨前線次第</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>2023-06-01 12:57:02+00:00</td>\n",
       "      <td>あれよな3.11ぐらいまではさ　ネタ無くて ニュースも　みーーーーーーんな 『藤井名人誕生』みたいな!!そう言うネタばかりだった 思うと3.11以降・・色々起き過ぎたわな;;思えばいがみ合いもここ等辺から原発・燃料そして熊本に洪水とコロナとロシアと・・そろそろ平穏期入って欲しいな。 昔はさ・・・</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>2023-06-01 06:09:02+00:00</td>\n",
       "      <td>台風２号の影響で3日にかけて東海3県は大雨の恐れ　土砂災害や浸水に警戒(メ〜テレ（名古屋テレビ）) #Yahooニュース</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>2023-06-01 06:08:47+00:00</td>\n",
       "      <td>まさかの2か月連続台風… それが無ければフローラ姫さんか、あやママさんで決まってたかな？と、思うけど、それも桃鉄。 　 でも、珍しいも。 水俣の畑に被害は無く… ほっとしました。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>2023-06-01 06:05:16+00:00</td>\n",
       "      <td>ハザードマップを確認 台風2号の北上にともなって、 日本の南にある前線の活動が活発になる見込みで、 東海3県では、今夜から3日にかけて雷をともなった非常に激しい雨が降るところがある見込みです。 自分が住む地域の災害の危険性を事前に確認し、避難の場所や方法を確認しておきましょう。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>2023-06-01 06:01:47+00:00</td>\n",
       "      <td>今後36時間で、300mmを超えるような予想のところもあります。この図はあくまでコンピュータの予想で、局地的にはそれよりも多くなる可能性もあります。 南西諸島から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>2023-06-01 06:00:16+00:00</td>\n",
       "      <td>本日夜から3日（土曜）明け方にかけて、大気の状態が非常に不安定となり、堺市においても激しい雨が降るおそれがあります。河川氾濫や土砂災害、道路の冠水などに注意が必要です。いざという時に慌てず行動できるよう、事前の備えを確認しておきましょう。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  \\\n",
       "6817  2023-06-01 12:59:40+00:00   \n",
       "6818  2023-06-01 12:58:00+00:00   \n",
       "6819  2023-06-01 12:57:26+00:00   \n",
       "6820  2023-06-01 12:57:16+00:00   \n",
       "6821  2023-06-01 12:57:02+00:00   \n",
       "...                         ...   \n",
       "7372  2023-06-01 06:09:02+00:00   \n",
       "7373  2023-06-01 06:08:47+00:00   \n",
       "7374  2023-06-01 06:05:16+00:00   \n",
       "7375  2023-06-01 06:01:47+00:00   \n",
       "7376  2023-06-01 06:00:16+00:00   \n",
       "\n",
       "                                                                                                                                                      text  \n",
       "6817                                                    美濃加茂は川あり山あり、平地あり。 自分の子供の頃は、太田地区の水害は毎年の事でしたが。 今は、だいぶ整理されたとは言え、局地的に降るとどうなるか読めませんよね。 大事に至らないのを祈るばかりです  \n",
       "6818                                                                      線状降水帯が発生した場合は、局地的に雨量が増えるおそれがあります。 西日本から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。  \n",
       "6819                 ◉台風2号接近のため注意喚起 気象庁の予報によると、明日6/2-6/3は台風接近に伴い、雨風が強まる可能性があります。明日、横浜は開港記念日。大変残念ですが安全が第一。都筑区の浸水ハザードマップ。川沿いエリアは特に注意してください。 #横浜市 の他エリアはこちらから  \n",
       "6820                                                        【あす2日　「災害級の大雨」の恐れ　線状降水帯発生の可能性も　避難の心構えを】tenkijp とは言うが台風の勢力は衰え始め、東京は関西程ではないのでは？と希望的観測。 全ては梅雨前線次第  \n",
       "6821  あれよな3.11ぐらいまではさ　ネタ無くて ニュースも　みーーーーーーんな 『藤井名人誕生』みたいな!!そう言うネタばかりだった 思うと3.11以降・・色々起き過ぎたわな;;思えばいがみ合いもここ等辺から原発・燃料そして熊本に洪水とコロナとロシアと・・そろそろ平穏期入って欲しいな。 昔はさ・・・  \n",
       "...                                                                                                                                                    ...  \n",
       "7372                                                                                          台風２号の影響で3日にかけて東海3県は大雨の恐れ　土砂災害や浸水に警戒(メ〜テレ（名古屋テレビ）) #Yahooニュース  \n",
       "7373                                                             まさかの2か月連続台風… それが無ければフローラ姫さんか、あやママさんで決まってたかな？と、思うけど、それも桃鉄。 　 でも、珍しいも。 水俣の畑に被害は無く… ほっとしました。  \n",
       "7374           ハザードマップを確認 台風2号の北上にともなって、 日本の南にある前線の活動が活発になる見込みで、 東海3県では、今夜から3日にかけて雷をともなった非常に激しい雨が降るところがある見込みです。 自分が住む地域の災害の危険性を事前に確認し、避難の場所や方法を確認しておきましょう。  \n",
       "7375                            今後36時間で、300mmを超えるような予想のところもあります。この図はあくまでコンピュータの予想で、局地的にはそれよりも多くなる可能性もあります。 南西諸島から東日本の広い範囲で、土砂災害、低い土地の浸水、河川の増水や氾濫に警戒してください。  \n",
       "7376                               本日夜から3日（土曜）明け方にかけて、大気の状態が非常に不安定となり、堺市においても激しい雨が降るおそれがあります。河川氾濫や土砂災害、道路の冠水などに注意が必要です。いざという時に慌てず行動できるよう、事前の備えを確認しておきましょう。  \n",
       "\n",
       "[560 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FSD_1777\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json\"\n",
    "dateFrom = \"2023-10-19T09:00:00+00:00\" #2023-10-19T18:58:41Z for 200 tweets\n",
    "dateTo = \"2023-10-19T18:00:00+00:00\"\n",
    "\n",
    "\"\"\"Load relevant fields of flood tags api json response\"\"\"\n",
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=True)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from FlagEmbedding import FlagReranker\n",
    "from pydantic.v1 import Field\n",
    "from typing import List, Dict, Any, Mapping\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents import Document\n",
    "#Cross encoder re rank\n",
    "class Custom_ja_Retriever(VectorStoreRetriever):\n",
    "    \"\"\"Implements re ranking of retriever output using cross encoder\"\"\"\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    search_type: str = \"similarity\"\n",
    "    search_kwargs: dict = Field(default_factory=dict)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"Extract relevant records using japanese embedding model\"\"\"\n",
    "\n",
    "        #Translate query to japanese \n",
    "        # query_ja = llmModels.bulk_translation(query, source_lan = \"en_XX\", output_lan= \"ja_XX\")\n",
    "        # query = query_ja[0]\n",
    "\n",
    "        #TRANSLATION -------------------------------------------------------------------\n",
    "        #Retrieve japanese records\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "\n",
    "        dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "        dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "        dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "        # Loading pandas dataframe from picke file\n",
    "        with open(dataPath, 'rb') as f:\n",
    "            data_eng = pickle.load(f)\n",
    "\n",
    "        df = pd.DataFrame(data_eng)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        # df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "        #Get data between thresholds\n",
    "        threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "        threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "        df = df[df['date'] >= threshold_datetime_lower]\n",
    "        df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "        #Remove duplicates\n",
    "        # df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "        #Covert date to string\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        data_eng = df\n",
    "\n",
    "        #Store japanese tweets in an array \n",
    "        ja_tweets = []\n",
    "        for doc in docs:\n",
    "            ja_tweets.append(doc.page_content)\n",
    "\n",
    "        #Get index of these extracted tweets\n",
    "        indexes = []\n",
    "        for tweet in ja_tweets:\n",
    "            indexes.append(data[data['text'] ==tweet].index[0])\n",
    "\n",
    "                #Extract english from exact location\n",
    "        docs_en = data_eng.loc[indexes].text.to_list()\n",
    "\n",
    "        #Chnage text to english \n",
    "        ind = 0\n",
    "        for doc in docs:\n",
    "            doc.page_content = docs_en[ind]\n",
    "            ind = ind+1\n",
    "\n",
    "\n",
    "        # Cross encoder re ranking ------------------------------------------\n",
    "        #For japanese query translation\n",
    "        query = \"Which areas are subject to flood warnings due to heavy rain?\"\n",
    "        cross_encoder = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "        pairs = []\n",
    "        for doc in docs:\n",
    "            pairs.append([query, doc.page_content])\n",
    "        scores = cross_encoder.compute_score(pairs)\n",
    "\n",
    "        i = 0\n",
    "        #Add scores to document\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"cross-encoder_score\"] = scores[i]\n",
    "            i = i+1\n",
    "        #Sort documents according to score\n",
    "        sorted_docs = sorted(docs, key=lambda doc: doc.metadata.get('cross-encoder_score'), reverse=True)\n",
    "\n",
    "        #Remove 20 worst performing docs from the list \n",
    "        sorted_docs = sorted_docs[:k_cross]\n",
    "\n",
    "        # #Re order according to long text re-order (Important context in start and end)\n",
    "        # reordering = LongContextReorder()\n",
    "        # reordered_docs = reordering.transform_documents(sorted_docs)\n",
    "\n",
    "        #Remove cross encoder score so re ordered context is back to its orignal form\n",
    "        for doc in sorted_docs:\n",
    "            doc.metadata.pop(\"cross-encoder_score\")\n",
    "\n",
    "        #Change updated docs back to docs\n",
    "        docs = sorted_docs\n",
    "\n",
    "        ##Extract japanese tweet texts -----------\n",
    "        # tweets_JA = []\n",
    "        # for doc in docs:\n",
    "        #     tweets_JA.append(doc.page_content)\n",
    "\n",
    "        # #Translate text feild to english\n",
    "        # # docs_en = llmModels.bulk_translation(tweets_JA)\n",
    "\n",
    "        # #Chnage text to english \n",
    "        # ind = 0\n",
    "        # for doc in docs:\n",
    "        #     doc.page_content = docs_en[ind]\n",
    "        #     ind = ind+1\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgeEmbeddings():\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    # model_name = \"BAAI/bge-m3\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVE 555 document(s) from langchain collection\n"
     ]
    }
   ],
   "source": [
    "embeddings = bgeEmbeddings()\n",
    "\n",
    "# embeddings = SentenceTransformer('pkshatech/GLuCoSE-base-ja')\n",
    "# embeddings = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"pkshatech/GLuCoSE-base-ja\")\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"pkshatech/GLuCoSE-base-ja\")\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "#Create a vector store\n",
    "# db = Chroma.Chroma(\"Langchain collection\",embeddings)\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings)\n",
    "if db._client.list_collections() != None:\n",
    "\n",
    "  for collection in db._client.list_collections():\n",
    "    ids = collection.get()['ids']\n",
    "    print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "    if len(ids): collection.delete(ids)\n",
    "\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all embeddings\n",
    "len(db._collection.get()['ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k': 20})\n",
    "query = \"\"\"Whihch locations have received evacuation orders due to Storm Babet?\"\"\"\n",
    "# k_cross = 50\n",
    "# retriever = Custom_ja_Retriever(vectorstore=db.as_retriever(search_kwargs={'k': 100}))\n",
    "\n",
    "# query = \"\"\"Which locations are likely to receive flood warnings due to heavy rain?\"\"\"\n",
    "docs = retriever.get_relevant_documents(query=query)\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='#StormBabet We are getting hit now Aberdeenshire! Some houses evacuated, flooding, bridges shut, roads flooded, flood gates shut, trees down, power outages. Look after each other, neighbours and animals. Stay safe!', metadata={'date': '2023-10-19 15:13:40+00:00'}), Document(page_content='Wild up here #thursdaymorning in #Aberdeen #aberdeenshire as #StormBabet sweeps the east, north with gale force winds &amp; heavy rainst now. Some schools, roads closed so travel safely everyone as #flooding bad in many riverside area(: #StormHour', metadata={'date': '2023-10-19 09:56:17+00:00'}), Document(page_content='STAY CLEAR OF FLOODED ROADS. Flooded roads often obscure potential hazards DO NOT DRIVE THROUGH ROAD CLOSURES Stay safe, turn around and seek an alternative route if travel is unavoidable! #Aberdeenshire #StormBabet', metadata={'date': '2023-10-19 10:45:27+00:00'}), Document(page_content='Scotland is preparing for flooding as #StormBabet approaches. Stay up to date: - Follow us for latest flooding info - for road conditions - for train travel - for severe weather advice - for weather forecasts #ReadyScotland', metadata={'date': '2023-10-19 13:42:00+00:00'}), Document(page_content='Over 300 homes in #Brechin, #Scotland, are being evacuated due to a rare red weather warning caused by #StormBabet. The region anticipates severe #flooding and life-threatening conditions. Red warnings are issued only when there is a high likelihood of dangerous weather that…', metadata={'date': '2023-10-19 13:05:40+00:00'}), Document(page_content='A serious situation is unfolding as evacuations notices are in effect as Storm #Babet brings life threatening flash flooding to parts of eastern #Scotland. #UKwx #flood #StormBabet', metadata={'date': '2023-10-19 12:42:18+00:00'}), Document(page_content='It’s NOT only parts of eastern Scotland that will be flooded! Wherever you are in the U.K. be aware across the next 48 hours. Babet has huge angry energy #StormBabet', metadata={'date': '2023-10-19 16:49:14+00:00'}), Document(page_content='Evacuations have just been ordered for the town of Brechin, Angus, alongside a severe flood warning . #Brechin #Angus #Scotlandweather #floodrisk #flooding', metadata={'date': '2023-10-19 11:52:03+00:00'}), Document(page_content=\"A reminder that during the next few days 'unprecidented' amounts of rainfall will bring extensive flooding to many Northern parts of Britain in association with #StormBabet - life threatening flooding is most likely across Eastern Scotland &amp; perhaps parts of Northern England.\", metadata={'date': '2023-10-19 12:32:23+00:00'}), Document(page_content='Conditions are really starting to deteriorate now in eastern Scotland with persistent rain setting in and severe-gales impacting coastal areas. Amber wind and rain warnings are now active, the red warning is valid from 18:00 this evening onwards into tomorrow. #StormBabet', metadata={'date': '2023-10-19 10:06:35+00:00'}), Document(page_content='Hundreds of people in Brechin in Scotland are being evacuated as exceptional rainfall from #StormBabet threatens to breach flood defences in the town. A red alert has been issued along with warnings of danger to life due to fast flowing floodwater.', metadata={'date': '2023-10-19 15:07:27+00:00'}), Document(page_content='There is a risk of flash flooding across Scotland as a result of Red, Amber and Yellow weather warnings for rain during #StormBabet. For further advice, please visit: Further information is also available on the Met Office website:', metadata={'date': '2023-10-19 10:55:08+00:00'}), Document(page_content='This is the South Esk river in Brechin as #StormBabet takes hold. Angus Council says 370 homes are to be evacuated due to the river’s rapidly rising levels. It’s been raining since this morning, heavily and steadily. The red warning for rain comes into force in just over an hour', metadata={'date': '2023-10-19 15:51:15+00:00'}), Document(page_content='This is the South Esk river in Brechin as #StormBabet takes hold. Angus Council says 370 homes are to be evacuated due to the river’s rapidly rising levels. It’s been raining since this morning, heavily and steadily. The red warning for rain comes into force in just over an hour', metadata={'date': '2023-10-19 17:44:42+00:00'}), Document(page_content='Evacuations notices are in effect as Storm #Babet brings life threatening flash flooding &amp; strong winds to parts of eastern #Scotland. #UKwx #flood #StormBabet', metadata={'date': '2023-10-19 16:41:12+00:00'}), Document(page_content=\"Severe weather warnings have been issued across the UK for #StormBabet - with extreme rain and very high winds forecast from this evening until Friday, particularly across Northern Ireland and Scotland. If you're worried about flooding in your area, these five steps can…\", metadata={'date': '2023-10-19 11:02:00+00:00'}), Document(page_content='RED ALERT WARNING In #Brechin, Scotland, more than 300 homes are being evacuated as a result of Storm Babet. The area expects significant flooding and potentially fatal circumstances. #Scotland #Brechin #Flooding #StormBabet #HeavyRain #Flood', metadata={'date': '2023-10-19 16:02:58+00:00'}), Document(page_content=\"fsb_policy: Severe weather warnings have been issued across the UK for #StormBabet - with extreme rain and very high winds forecast from this evening until Friday, particularly across Northern Ireland and Scotland. If you're worried about flooding…\", metadata={'date': '2023-10-19 11:07:17+00:00'}), Document(page_content='UK weather: Storm #Babet forces #evacuations in red #alert area of #Brechin due to #floods', metadata={'date': '2023-10-19 15:27:16+00:00'}), Document(page_content=\"#StormBabet is causing disruption to Scotland's railway. No trains are running on a number of routes in the north and east of the country due to high winds and extremely heavy rain. A number of weather warnings, including a RED 'danger to life' warning are in place.\", metadata={'date': '2023-10-19 15:14:04+00:00'}), Document(page_content='Don’t think that #StormBabet is only going to be hazardous in that highlighted red zone of eastern Scotland, her impacts (flood/high wind gusts) will be felt widely through Friday into Saturday. Midlands regions of England particularly earmarked!', metadata={'date': '2023-10-19 09:56:47+00:00'}), Document(page_content='BREAKING: Storm Babet forces evacuations in red alert area of Brechin due to floods Read more', metadata={'date': '2023-10-19 11:32:39+00:00'}), Document(page_content='Mandatory evacuations have just been issued for the town of Brechin, Angus, alongside a severe flood warning . #Brechin #Angus #Scotlandweather #floodrisk #flooding', metadata={'date': '2023-10-19 11:43:43+00:00'}), Document(page_content='UK weather: Storm Babet forces evacuations in red alert area of Angus due to floods', metadata={'date': '2023-10-19 17:45:02+00:00'}), Document(page_content='Storm Babet: Hundreds evacuated as torrential rain threatens flood defences. Residents of about 370 homes in Brechin are told to leave. #StormBabet #Evacuation #FloodDefences', metadata={'date': '2023-10-19 15:18:59+00:00'}), Document(page_content='Have a listen to Teresa Mannion on behalf of . #StormBabet has left a severe flooding impact especially in #Midleton. Scotland had a red warning for #StormBabet #traveltips', metadata={'date': '2023-10-19 11:03:58+00:00'}), Document(page_content='Breaking: Flood warning issued along the River Don in Aberdeenshire', metadata={'date': '2023-10-19 17:59:34+00:00'}), Document(page_content='#BREAKING: All residents in Brechin, Scotland, have been asked to evacuate due to Storm Babet floods', metadata={'date': '2023-10-19 11:42:51+00:00'}), Document(page_content='Storm #Babet, which previously caused severe flooding in Ireland, has reached #Britain The video shows #Stonehaven Harbor in #Scotland.', metadata={'date': '2023-10-19 16:07:36+00:00'}), Document(page_content='Flood gates going up in Brechin today. Hundreds of homes are being evacuated here as heavy rain and high winds continue to hit the area. I’ll be on shortly with more.', metadata={'date': '2023-10-19 15:42:35+00:00'}), Document(page_content='Extreme rainfall from Storm #Babet could lead to major flash flooding in parts of Scotland. Latest scenes:', metadata={'date': '2023-10-19 16:27:45+00:00'}), Document(page_content='#StormBabet will be relevant from now until about 6am on Saturday. Bringing wind and very heavy rain across areas of Scotland and northern and central England. Image 1 - Today Image 2 - Friday Image 3 - Saturday See weather warning page for details...', metadata={'date': '2023-10-19 13:59:47+00:00'}), Document(page_content='Our 5 News reporter Alan Jenkins is battling high winds and heavy rain in Montrose in Scotland. There‘s a red weather warning as Storm Babet hits east Scotland and other parts of the UK. #Weather #StormBabet #RedWarning #Storm', metadata={'date': '2023-10-19 15:37:32+00:00'}), Document(page_content=\"Entire Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for 70mph gale force winds and heavy rain set to hit at 6pm\", metadata={'date': '2023-10-19 12:16:06+00:00'}), Document(page_content=\"Entire Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for 70mph gale force winds and heavy rain set to hit at 6pm\", metadata={'date': '2023-10-19 11:55:47+00:00'}), Document(page_content=\"Entire Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for 70mph gale force winds and heavy rain set to hit at 6pm\", metadata={'date': '2023-10-19 11:58:05+00:00'}), Document(page_content=\"Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for 70mph gale force winds and heavy rain set to hit at 6pm\", metadata={'date': '2023-10-19 12:46:49+00:00'}), Document(page_content=\"Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for 70mph gale force winds and heavy rain set to hit at 6pm\", metadata={'date': '2023-10-19 13:03:47+00:00'}), Document(page_content='The reality of rapid river level rise due to #StormBabet: footage from at today, &amp; the water level just downstream (Gella bridge on the River Sth Esk). #Brechin residents being evacuated. #rainfall #runoff #storm', metadata={'date': '2023-10-19 17:56:11+00:00'}), Document(page_content=\"Entire Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red al... via\", metadata={'date': '2023-10-19 12:32:50+00:00'}), Document(page_content=\"#StormBabet: We're preparing for strong winds and heavy rain across the South East and East of England over the next few days. Check our website for advice and updates about your power:\", metadata={'date': '2023-10-19 15:20:02+00:00'}), Document(page_content='Storm Babet hits Scotland LIVE Flood warnings and alerts in place across country', metadata={'date': '2023-10-19 09:12:10+00:00'}), Document(page_content='Warnings increased as Storm Babet batters England, with homes and businesses at risk of flooding and danger to life. Stay safe! #UKWeather #StormBabet', metadata={'date': '2023-10-19 12:30:31+00:00'}), Document(page_content='and showing the ABSOLUTE DEVASTATION in Arbroath today with #StormBabet… this storm is wrecking the area and flooding the place and THIS is what photo goes on the news….', metadata={'date': '2023-10-19 12:40:20+00:00'}), Document(page_content='stay safe up there please UK weather: Storm Babet forces evacuations in red alert area of Brechin due to floods', metadata={'date': '2023-10-19 12:52:38+00:00'}), Document(page_content=\"Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for... via\", metadata={'date': '2023-10-19 14:00:32+00:00'}), Document(page_content=\"Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for... via\", metadata={'date': '2023-10-19 13:20:07+00:00'}), Document(page_content=\"Scottish town is evacuated as Britain braces for Storm Babet: Flood barriers are set up, trains are cancelled and 'do not travel' warnings are issued - as Met Office expands red alert for... via\", metadata={'date': '2023-10-19 14:06:55+00:00'}), Document(page_content='BREAKING: Residents in Brechin are being asked to evacuate amid concerns over flooding due to Storm Babet More to follow', metadata={'date': '2023-10-19 11:43:13+00:00'}), Document(page_content='Breaking 350 homes in Brechin, Scotland told to evacuate after warning of extensive flooding and risk to life from Storm Babet Reports BBC', metadata={'date': '2023-10-19 12:45:06+00:00'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m ja_tweets:\n\u001b[0;32m---> 35\u001b[0m     indexes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mtweet\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#Search for indexes over the english data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(indexes))\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/pandas/core/indexes/base.py:5366\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[1;32m   5364\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[1;32m   5365\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[0;32m-> 5366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   5369\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[1;32m   5370\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m   5371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "#FSD_1555 --- To track English tweets wothout translation \n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data_eng = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data_eng)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "#Remove duplicates\n",
    "# df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "data_eng = df\n",
    "\n",
    "#Store japanese tweets in an array \n",
    "ja_tweets = []\n",
    "for doc in docs:\n",
    "    ja_tweets.append(doc.page_content)\n",
    "\n",
    "#Get index of these extracted tweets\n",
    "indexes = []\n",
    "for tweet in ja_tweets:\n",
    "    indexes.append(data[data['text'] ==tweet].index[0])\n",
    "\n",
    "#Search for indexes over the english data\n",
    "print(len(indexes))\n",
    "print(data_eng.loc[indexes].text.to_list())\n",
    "# with open(\"Output.txt\", \"w\") as text_file:\n",
    "#     text_file.write(data_eng.loc[indexes].text.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_tweets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_string = ['String 1', 'String 2', 'String 3', 'String 4', 'String 5']\n",
    "\n",
    "#Extract japanese tweet texts\n",
    "tweets_JA = []\n",
    "ind = 0\n",
    "for doc in docs:\n",
    "    doc.page_content = eng_string[ind]\n",
    "    ind = ind+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing UAE embeddings\n",
    "from scipy import spatial\n",
    "input = \"Which locations have recieved an evacuation order?\"\n",
    "query = embeddings.embed_query (input)\n",
    "docs = retriever.get_relevant_documents(query=input)\n",
    "\n",
    "doc_vecs = []\n",
    "for doc in docs:\n",
    "    doc_vecs.append(embeddings.embed_query (doc.page_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = []\n",
    "for dv in doc_vecs:\n",
    "    cosine_sim.append(spatial.distance.cosine(query, dv))\n",
    "\n",
    "    # print(len(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Aberdeenshire\"\"\"\n",
    "# [d[1] for d in db.similarity_search_with_score(query, k=300)]\n",
    "outp =  db.similarity_search_with_score(query, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for element in cosine_sim:\n",
    "    if element < 0.58:\n",
    "        print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New embeddging model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "input_texts = [\n",
    "    \"what is the capital of China?\",\n",
    "    \"how to implement quick sort in python?\",\n",
    "    \"Beijing\",\n",
    "    \"sorting algorithms\"\n",
    "]\n",
    "\n",
    "model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = outputs.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(t).tolist() for t in texts]\n",
    "\n",
    "\n",
    "embeddings = MyEmbeddings()\n",
    "\n",
    "# splitter = SemanticChunker(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom embeddings with non-default embedding model\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        ##\n",
    "        #New embeddging model\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "        model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        batch_dict = tokenizer(input, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]\n",
    "                \n",
    "        return embeddings.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "custom_embeddings=MyEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "print(documents)\n",
    "\n",
    "# db = Chroma.Chroma.from_documents(documents,\n",
    "#                                   custom_embeddings,\n",
    "#                                   collection_metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
