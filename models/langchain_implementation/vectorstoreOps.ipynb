{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_community.vectorstores import chroma as Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from Text_preprocessing import Text_preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions \n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FSD_1555\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_Japan_06_15.pkl\"\n",
    "# dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "#Remove duplicates\n",
    "# df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FSD_1777\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json\"\n",
    "dateFrom = \"2023-10-19T09:00:00+00:00\" #2023-10-19T18:58:41Z for 200 tweets\n",
    "dateTo = \"2023-10-19T18:00:00+00:00\"\n",
    "\n",
    "\"\"\"Load relevant fields of flood tags api json response\"\"\"\n",
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=True)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from FlagEmbedding import FlagReranker\n",
    "from pydantic.v1 import Field\n",
    "from typing import List, Dict, Any, Mapping\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents import Document\n",
    "#Cross encoder re rank\n",
    "class Custom_ja_Retriever(VectorStoreRetriever):\n",
    "    \"\"\"Implements re ranking of retriever output using cross encoder\"\"\"\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    search_type: str = \"similarity\"\n",
    "    search_kwargs: dict = Field(default_factory=dict)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"Extract relevant records using japanese embedding model\"\"\"\n",
    "\n",
    "        #Translate query to japanese \n",
    "        # query_ja = llmModels.bulk_translation(query, source_lan = \"en_XX\", output_lan= \"ja_XX\")\n",
    "        # query = query_ja[0]\n",
    "\n",
    "        #TRANSLATION -------------------------------------------------------------------\n",
    "        #Retrieve japanese records\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "\n",
    "        dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "        dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "        dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "        # Loading pandas dataframe from picke file\n",
    "        with open(dataPath, 'rb') as f:\n",
    "            data_eng = pickle.load(f)\n",
    "\n",
    "        df = pd.DataFrame(data_eng)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        # df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "        #Get data between thresholds\n",
    "        threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "        threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "        df = df[df['date'] >= threshold_datetime_lower]\n",
    "        df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "        #Remove duplicates\n",
    "        # df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "        #Covert date to string\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        data_eng = df\n",
    "\n",
    "        #Store japanese tweets in an array \n",
    "        ja_tweets = []\n",
    "        for doc in docs:\n",
    "            ja_tweets.append(doc.page_content)\n",
    "\n",
    "        #Get index of these extracted tweets\n",
    "        indexes = []\n",
    "        for tweet in ja_tweets:\n",
    "            indexes.append(data[data['text'] ==tweet].index[0])\n",
    "\n",
    "                #Extract english from exact location\n",
    "        docs_en = data_eng.loc[indexes].text.to_list()\n",
    "\n",
    "        #Chnage text to english \n",
    "        ind = 0\n",
    "        for doc in docs:\n",
    "            doc.page_content = docs_en[ind]\n",
    "            ind = ind+1\n",
    "\n",
    "\n",
    "        # Cross encoder re ranking ------------------------------------------\n",
    "        #For japanese query translation\n",
    "        query = \"Which areas are subject to flood warnings due to heavy rain?\"\n",
    "        cross_encoder = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "        pairs = []\n",
    "        for doc in docs:\n",
    "            pairs.append([query, doc.page_content])\n",
    "        scores = cross_encoder.compute_score(pairs)\n",
    "\n",
    "        i = 0\n",
    "        #Add scores to document\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"cross-encoder_score\"] = scores[i]\n",
    "            i = i+1\n",
    "        #Sort documents according to score\n",
    "        sorted_docs = sorted(docs, key=lambda doc: doc.metadata.get('cross-encoder_score'), reverse=True)\n",
    "\n",
    "        #Remove 20 worst performing docs from the list \n",
    "        sorted_docs = sorted_docs[:k_cross]\n",
    "\n",
    "        # #Re order according to long text re-order (Important context in start and end)\n",
    "        # reordering = LongContextReorder()\n",
    "        # reordered_docs = reordering.transform_documents(sorted_docs)\n",
    "\n",
    "        #Remove cross encoder score so re ordered context is back to its orignal form\n",
    "        for doc in sorted_docs:\n",
    "            doc.metadata.pop(\"cross-encoder_score\")\n",
    "\n",
    "        #Change updated docs back to docs\n",
    "        docs = sorted_docs\n",
    "\n",
    "        ##Extract japanese tweet texts -----------\n",
    "        # tweets_JA = []\n",
    "        # for doc in docs:\n",
    "        #     tweets_JA.append(doc.page_content)\n",
    "\n",
    "        # #Translate text feild to english\n",
    "        # # docs_en = llmModels.bulk_translation(tweets_JA)\n",
    "\n",
    "        # #Chnage text to english \n",
    "        # ind = 0\n",
    "        # for doc in docs:\n",
    "        #     doc.page_content = docs_en[ind]\n",
    "        #     ind = ind+1\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgeEmbeddings():\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    # model_name = \"BAAI/bge-m3\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = bgeEmbeddings()\n",
    "\n",
    "# embeddings = SentenceTransformer('pkshatech/GLuCoSE-base-ja')\n",
    "# embeddings = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"pkshatech/GLuCoSE-base-ja\")\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"pkshatech/GLuCoSE-base-ja\")\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "#Create a vector store\n",
    "# db = Chroma.Chroma(\"Langchain collection\",embeddings)\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings)\n",
    "if db._client.list_collections() != None:\n",
    "\n",
    "  for collection in db._client.list_collections():\n",
    "    ids = collection.get()['ids']\n",
    "    print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "    if len(ids): collection.delete(ids)\n",
    "\n",
    "db = Chroma.Chroma.from_documents(documents,\n",
    "                                  embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all embeddings\n",
    "len(db._collection.get()['ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k': 50})\n",
    "query = \"\"\"Act as a location extrator and extract all the relevant locations receiving flood warnings\"\"\"\n",
    "# k_cross = 50\n",
    "# retriever = Custom_ja_Retriever(vectorstore=db.as_retriever(search_kwargs={'k': 100}))\n",
    "\n",
    "# query = \"\"\"Which locations are likely to receive flood warnings due to heavy rain?\"\"\"\n",
    "docs = retriever.get_relevant_documents(query=query)\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FSD_1555 --- To track English tweets wothout translation \n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_06_15.pkl\"\n",
    "dateFrom = \"2023-06-01 06:00:00+00:00\" \n",
    "dateTo = \"2023-06-01 13:00:00+00:00\" \n",
    "\n",
    "# Loading pandas dataframe from picke file\n",
    "with open(dataPath, 'rb') as f:\n",
    "    data_eng = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data_eng)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "#Get data between thresholds\n",
    "threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "df = df[df['date'] >= threshold_datetime_lower]\n",
    "df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "#Remove duplicates\n",
    "# df_new  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "#Covert date to string\n",
    "df['date'] = df['date'].astype(str)\n",
    "data_eng = df\n",
    "\n",
    "#Store japanese tweets in an array \n",
    "ja_tweets = []\n",
    "for doc in docs:\n",
    "    ja_tweets.append(doc.page_content)\n",
    "\n",
    "#Get index of these extracted tweets\n",
    "indexes = []\n",
    "for tweet in ja_tweets:\n",
    "    indexes.append(data[data['text'] ==tweet].index[0])\n",
    "\n",
    "#Search for indexes over the english data\n",
    "print(len(indexes))\n",
    "print(data_eng.loc[indexes].text.to_list())\n",
    "# with open(\"Output.txt\", \"w\") as text_file:\n",
    "#     text_file.write(data_eng.loc[indexes].text.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_tweets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_string = ['String 1', 'String 2', 'String 3', 'String 4', 'String 5']\n",
    "\n",
    "#Extract japanese tweet texts\n",
    "tweets_JA = []\n",
    "ind = 0\n",
    "for doc in docs:\n",
    "    doc.page_content = eng_string[ind]\n",
    "    ind = ind+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing UAE embeddings\n",
    "from scipy import spatial\n",
    "input = \"Which locations have recieved an evacuation order?\"\n",
    "query = embeddings.embed_query (input)\n",
    "docs = retriever.get_relevant_documents(query=input)\n",
    "\n",
    "doc_vecs = []\n",
    "for doc in docs:\n",
    "    doc_vecs.append(embeddings.embed_query (doc.page_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = []\n",
    "for dv in doc_vecs:\n",
    "    cosine_sim.append(spatial.distance.cosine(query, dv))\n",
    "\n",
    "    # print(len(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Aberdeenshire\"\"\"\n",
    "# [d[1] for d in db.similarity_search_with_score(query, k=300)]\n",
    "outp =  db.similarity_search_with_score(query, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for element in cosine_sim:\n",
    "    if element < 0.58:\n",
    "        print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New embeddging model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "input_texts = [\n",
    "    \"what is the capital of China?\",\n",
    "    \"how to implement quick sort in python?\",\n",
    "    \"Beijing\",\n",
    "    \"sorting algorithms\"\n",
    "]\n",
    "\n",
    "model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = outputs.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(t).tolist() for t in texts]\n",
    "\n",
    "\n",
    "embeddings = MyEmbeddings()\n",
    "\n",
    "# splitter = SemanticChunker(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom embeddings with non-default embedding model\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        ##\n",
    "        #New embeddging model\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "        model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        batch_dict = tokenizer(input, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]\n",
    "                \n",
    "        return embeddings.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "custom_embeddings=MyEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model\n",
    "\n",
    "documents = []\n",
    "loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "documents.extend(loader.load())\n",
    "print(documents)\n",
    "\n",
    "# db = Chroma.Chroma.from_documents(documents,\n",
    "#                                   custom_embeddings,\n",
    "#                                   collection_metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
