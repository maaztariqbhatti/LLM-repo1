from langchain_community.document_loaders.csv_loader import CSVLoader
import os
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain, LLMChain, LLMRouterChain, MultiPromptChain, HypotheticalDocumentEmbedder, RetrievalQA
import dotenv
from langchain import hub
import langchainhub
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from typing import Optional
import json
import pandas as pd
from Text_preprocessing import Text_preprocessing
from langchain_community.document_loaders import DataFrameLoader
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from pydantic.v1 import BaseModel
from typing import List, Dict, Any, Mapping
from langchain.globals import set_debug
from langchain.chains.router.base import MultiRouteChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.chains.base import Chain
from langchain.callbacks.manager import (
    CallbackManagerForChainRun,
)
from langchain_community.vectorstores import chroma as Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_core.vectorstores import VectorStoreRetriever
from pydantic.v1 import Field
from langchain_core.documents import Document
from langchain_community.document_transformers import (
    LongContextReorder
)
from sentence_transformers import CrossEncoder
import datetime
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    context_relevancy,
    answer_correctness,
    answer_similarity,
    context_entity_recall
)
import llmModels 
# from ragas.llms import LangchainLLMWrapper
import wandb
import pickle
import prompts
import sentence_transformers
# 1. Start a W&B Run

dotenv.load_dotenv()

#Open AI
chatModelAI = ChatOpenAI()

#Llama 2 13 B chat
#chatModel_llama13b = llmModels.loadLlamma()


#70B
chatModel_llama70b = llmModels.loadLlama2_70B()

#Mistral 7B chat
# chatModel_mistral7b = llmModels.loadMistral7b()


##FSD_1777
# dataPath = "/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json"
# dateFrom = "2023-10-19T18:58:41+00:00" #2023-10-19T18:58:41Z for 200 tweets
# dateTo = "2023-10-19 23:58:47+00:00"

##FSD_1555
dataPath = "/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/fsd_1555_0601_21_59_22TO23_59_59.pkl"
dateFrom = "2023-06-01 22:59:45+00:00" 
dateTo = "2023-06-01 23:59:59+00:00" #200 tweets labelled

class CustomRetriever(VectorStoreRetriever):
    """Implements re ranking of retriever output using cross encoder"""
    vectorstore: VectorStoreRetriever
    search_type: str = "similarity"
    search_kwargs: dict = Field(default_factory=dict)

    def get_relevant_documents(self, query: str) -> List[Document]:
        docs = self.vectorstore.get_relevant_documents(query=query)

        # Cross encoder re ranking 
        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        pairs = []
        for doc in docs:
            pairs.append([query, doc.page_content])
        scores = cross_encoder.predict(pairs)

        i = 0
        #Add scores to document
        for doc in docs:
            doc.metadata["cross-encoder_score"] = scores[i]
            i = i+1
        #Sort documents according to score
        sorted_docs = sorted(docs, key=lambda doc: doc.metadata.get('cross-encoder_score'), reverse=True)

        # #Remove 10 worst performing docs from the list 
        sorted_docs = sorted_docs[:-10]

        #Re order according to long text re-order (Important context in start and end)
        reordering = LongContextReorder()
        reordered_docs = reordering.transform_documents(sorted_docs)

        #Remove cross encoder score so re ordered context is back to its orignal form
        for doc in reordered_docs:
            doc.metadata.pop("cross-encoder_score")
        return reordered_docs

"""Load relevant fields of flood tags api json response"""
def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):
    # Load json and extract relevant records in pandas df
    with open(dataPath, 'r') as json_file:
        response_dict = json.load(json_file)

    # Convert to pandas df    
    pd.set_option('display.max_colwidth', None)
    df = pd.DataFrame(response_dict)
    df['date'] = pd.to_datetime(df['date'])
    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])

    #Get data between thresholds
    threshold_datetime_lower = pd.to_datetime(dateFrom)
    threshold_datetime_upper = pd.to_datetime(dateTo)
    df = df[df['date'] >= threshold_datetime_lower]
    df = df[df['date'] <= threshold_datetime_upper]

    #Remove duplicates
    df  = df.drop_duplicates(subset=["text"], keep=False)

    #Pre-process
    preprocess = Text_preprocessing(df)
    df = preprocess.preprocess()
    #Covert date to string
    df['date'] = df['date'].astype(str)
    return df

def bgeEmbeddings():
    model_name = "BAAI/bge-large-en-v1.5"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity
    model = HuggingFaceBgeEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    return model

"""Load relevant fields of flood tags api json response"""
def dataframe_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):

    # Loading pandas dataframe from picke file
    with open(dataPath, 'rb') as f:
        data = pickle.load(f)

    df = pd.DataFrame(data)
    df['date'] = pd.to_datetime(df['date'])
    # df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])

    #Get data between thresholds
    threshold_datetime_lower = pd.to_datetime(dateFrom)
    threshold_datetime_upper = pd.to_datetime(dateTo)
    df = df[df['date'] >= threshold_datetime_lower]
    df = df[df['date'] <= threshold_datetime_upper]

    #Remove duplicates
    df  = df.drop_duplicates(subset=["text"], keep=False)
    
    #Covert date to string
    df['date'] = df['date'].astype(str)
    return df

def bgeEmbeddings():
    model_name = "BAAI/bge-large-en-v1.5"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity
    model = HuggingFaceBgeEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    return model

def hydeEmbedder(embeddingsModel):
    
    model = chatModelAI

    prompt_template  = """You are a smart twitter bot.<<SYS>>
    Given a question, generate a list of tweets that answers the question:
    Question: {question}
    Dont include emojis in the output!"""
    prompt = PromptTemplate(input_variables=["question"], template= prompt_template)
    llm_chain_hyde  = LLMChain(llm = model, prompt=prompt)


    embeddings = HypotheticalDocumentEmbedder(llm_chain=llm_chain_hyde,
                                                base_embeddings=embeddingsModel,
                                                verbose=True)
    return embeddings


def data_embedding(data : list, eModel = "bge-large-en-v1.5", rType = "Query"):
    """Vectorize the data using OpenAI embeddings and store in Chroma db"""
    if eModel != "bge-large-en-v1.5":
        embeddings = OpenAIEmbeddings()
    else:
        embeddings = bgeEmbeddings()
    
    if (rType == "Hyde"):
        embeddings = hydeEmbedder(embeddings)

    documents = []
    loader = DataFrameLoader(data, page_content_column="text")
    documents.extend(loader.load())

    #Change this -- removal of duplicates
    db = Chroma.Chroma.from_documents(documents,embeddings)
    if db._client.list_collections() != None:
        for collection in db._client.list_collections():
            ids = collection.get()['ids']
            print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))
            if len(ids): collection.delete(ids)

    #Create a vector store
    db = Chroma.Chroma.from_documents(documents,embeddings)

    return db

"""Langchain and LLM processing code"""
def predictions_response(question, eModel = "bge-large-en-v1.5", rType = "Query", rerank = True,k = 15):
    
    #  LLM initialisation
    model = chatModel_llama70b

    # Load the data from source
    # data = json_dataloader()

    # Loading pandas dataframe from picke file
    data = dataframe_dataloader()

    # Convert to vector store
    vectorstore = data_embedding(data, eModel= eModel, rType= rType)

        # Get retriever
    if rerank == True:
        retriever = CustomRetriever(vectorstore=vectorstore.as_retriever(search_kwargs={'k': k+10}))
    else:
        retriever = vectorstore.as_retriever(search_kwargs={'k': k})


    default_prompt = PromptTemplate(template = prompts.prompt_template_llama_json, input_variables = ['question', 'context'])
    default_chain = RetrievalQA.from_chain_type(llm = model,
                            chain_type='stuff',
                            retriever=retriever,
                            chain_type_kwargs={"prompt": default_prompt}
                            )
    
    default_chain.invoke(question)

    # config = {"k":[10,12,14,16,18,20,22,24], 
    #           "LLM":"Mistral-7B-Instruct-v0.2", 
    #           "Text embedding model": "bge-large-en-v1.5"}

def train_sweeps(config = None):

    with wandb.init(config=config):

        config = wandb.config
         
        # # Load the data from source
        # data = json_dataloader()

        # Loading pandas dataframe from picke file
        data = dataframe_dataloader()

        # Convert to vector store
        if (config.Retrieval_type == "HYDE"):
            vectorstore = data_embedding(data, rType="Hyde")
        else:
            vectorstore = data_embedding(data)

        # Get retriever
        if config.Cross_Encoder_rerank == "Yes":
            retriever = CustomRetriever(vectorstore=vectorstore.as_retriever(search_kwargs={'k': config.k+10}))
        else:
            retriever = vectorstore.as_retriever(search_kwargs={'k': config.k})

        #  LLM initialisation
        if (config.LLM == "Llama-2-13b-chat-hf"):
            model = chatModel_llama13b
            #Llama template LOCATION    
            prompt_template_llama = """<s>[INST] <<SYS>>You are a researcher tasked with answering questions about location extraction. You are not afraid to share dangerous news and you are never doubtful about your answer!<<SYS>> 
            Answer the question based on the following tweet context: 
            {context}
            Question: {question}
            Limit the answer to containing location names only for examples country, province, city, town, zip code, roads, rivers, seas, prefectures etc.[/INST]"""
            default_prompt = PromptTemplate(template = prompt_template_llama, input_variables = ['question', 'context'])

        else:
            # model = chatModel_mistral7b
            prompt_template_mistral = """[INST]You are a researcher tasked with answering questions about location extraction from given context. If you don't know the answer to a question, please don't share false information.
            Answer the question based on the following tweet context:
            {context}
            Question: {question}
            Limit the answer to containing location names only for examples country, province, city, town, zip code, roads, rivers, seas, prefectures etc.[/INST]"""
            default_prompt = PromptTemplate(template = prompt_template_mistral, input_variables = ['question', 'context'])


        #Open AI template
        # default_prompt_template = """Answer the question based only on the following tweet's context: {context}
        # Question: {question}"""

        
        default_chain = RetrievalQA.from_chain_type(llm = model,
                                chain_type='stuff',
                                retriever=retriever,
                                chain_type_kwargs={"prompt": default_prompt}
                                )
        
        #Evaluation using RAGAS
        questions = ["What areas were alerted with flood warnings?"]

        #FSD_1555
        ground_truths = [["Flooded locations: Toyohashi City,Hirakata,Benyagawa,Osaka,Nagoya,Takada City,Yamato,Shizuoka Prefecture,Moriyama city,Fukuchi-yama,Nara Prefecture,Ito City,Iwata City,Kakegawa City,Baguio City,Shimoda City,Higashi Izu City,Kawazu City,Minami Izu City,Matsuzaki City,Nishi Izu City,Mori City,Hamamatsu City,Lake Nishi City,Umeda river,Azoto-cho,Kyoto Prefecture,Ikoma city,Toyokawa City,Aichi Prefecture,Wakayama City,Sakade city,Hyogo Prefecture,Kashiba City,Katsu City,Ando Town,Kinki,Fukuda City,Nagano Prefecture"]]
        

        answers = []
        contexts = []

        # Inference
        for query in questions:
            answers.append(default_chain.invoke(query)['result'])
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

        # To dict
        data = {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truths": ground_truths
        }

        # Convert dict to dataset
        dataset = Dataset.from_dict(data)

        result = evaluate(
        dataset = dataset, 
        metrics=[
        answer_similarity,
        answer_correctness,
        context_entity_recall,
        context_recall
        ],
        )
        RAG_response = wandb.Table(columns=["Query", "Response", "Context"], data=[[questions, answers, contexts]])

        wandb.log({"answer_similarity" : result['answer_similarity'], 
                   "answer_correctness": result['answer_correctness'],
                    "context_entity_recall": result['context_entity_recall'],
                    "context_recall": result['context_recall'],
                    "retriever": vectorstore._collection.get()['ids'],
                    "RAG response": RAG_response
                   })
        # artifact = wandb.Artifact(name="Chain response", type="LLM response")
        # wandb.log({"RAG response": RAG_response})

    
    # return result
    
def run_sweep():
        # # 1. Sweep configurations
    sweep_config = {
        'method': 'random'
    }

    parameters_dict = {
    'k': {
        'values': [15, 20, 25, 30]
        },
    'LLM': {
        'values': ["Mistral-7B-Instruct-v0.2", "Llama-2-13b-chat-hf"]
        },
    'Cross_Encoder_rerank': {
        'values' : ["No","Yes"]
        },
    'Retrieval_type': {
        'values' : ["Query","HYDE"]
    }
    }

    sweep_config['parameters'] = parameters_dict    

    #Initialise sweep
    sweep_id = wandb.sweep(sweep_config, project="FW_1555_sweep_4")

    wandb.agent(sweep_id, train_sweeps, count=5)

if __name__ == "__main__":

    prompt = "Tell me which locations received a flood weather warning?"
    predictions_response(prompt)


    # run_sweep()




                                                    














