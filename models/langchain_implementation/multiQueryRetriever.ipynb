{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import openai\n",
    "from langchain.chains import LLMChain, LLMRouterChain, MultiPromptChain, HypotheticalDocumentEmbedder, RetrievalQA\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Optional\n",
    "import json\n",
    "import pandas as pd\n",
    "from Text_preprocessing import Text_preprocessing\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from typing import List, Dict, Any, Mapping\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "from langchain_community.vectorstores import chroma as Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from pydantic.v1 import Field\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder\n",
    ")\n",
    "from sentence_transformers import CrossEncoder\n",
    "import llmModels \n",
    "import wandb\n",
    "import pickle\n",
    "import prompts\n",
    "import groundTruths\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "## Llama 3 8B\n",
    "chatModel_llama3_8B = llmModels.loadLlama3_8B()\n",
    "##FSD_1777\n",
    "dataPath = \"/home/mbhatti/mnt/d/LLM-repo1/models/langchain_implementation/FSD1777_Oct23.json\"\n",
    "dateFrom = \"2023-10-19T09:00:00+00:00\" #2023-10-19T18:58:41Z for 200 tweets\n",
    "dateTo = \"2023-10-19T18:00:00+00:00\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load relevant fields of flood tags api json response\"\"\"\n",
    "def json_dataloader(dataPath = dataPath, dateFrom = dateFrom, dateTo = dateTo):\n",
    "    # Load json and extract relevant records in pandas df\n",
    "    with open(dataPath, 'r') as json_file:\n",
    "        response_dict = json.load(json_file)\n",
    "\n",
    "    # Convert to pandas df    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(response_dict)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['id','tag_class', 'source', 'lang', 'urls','locations'])\n",
    "\n",
    "    #Get data between thresholds\n",
    "    threshold_datetime_lower = pd.to_datetime(dateFrom)\n",
    "    threshold_datetime_upper = pd.to_datetime(dateTo)\n",
    "    df = df[df['date'] >= threshold_datetime_lower]\n",
    "    df = df[df['date'] <= threshold_datetime_upper]\n",
    "\n",
    "    #Remove duplicates\n",
    "    df  = df.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "    #Pre-process\n",
    "    preprocess = Text_preprocessing(df)\n",
    "    df = preprocess.preprocess()\n",
    "    #Covert date to string\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    return df\n",
    "\n",
    "def bgeEmbeddings():\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def data_embedding(data : list, eModel = \"bge-large-en-v1.5\", rType = \"Query\", metric = \"L2\"):\n",
    "    \"\"\"Vectorize the data using OpenAI embeddings and store in Chroma db\"\"\"\n",
    "    if eModel != \"bge-large-en-v1.5\":\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    else:\n",
    "        embeddings = bgeEmbeddings()\n",
    "    \n",
    "    # if (rType == \"Hyde\"):\n",
    "    #     embeddings = multiGen_hydeEmbedder(embeddings)\n",
    "\n",
    "    documents = []\n",
    "    loader = DataFrameLoader(data, page_content_column=\"text\")\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "    #Change this -- removal of duplicates\n",
    "    db = Chroma.Chroma.from_documents(documents,embeddings)\n",
    "    # if db._client.list_collections() != None:\n",
    "    #     for collection in db._client.list_collections():\n",
    "    #         ids = collection.get()['ids']\n",
    "    #         print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "    #         if len(ids): collection.delete(ids)\n",
    "\n",
    "    # #Create a vector store\n",
    "    # if metric == \"cosine\":\n",
    "    #     db = Chroma.Chroma.from_documents(documents,embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "    # else:\n",
    "    #     db = Chroma.Chroma.from_documents(documents,embeddings)\n",
    "    print(len(db._collection.get()['ids']))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVE 480 document(s) from langchain collection\n"
     ]
    }
   ],
   "source": [
    "if vectorstore._client.list_collections() != None:\n",
    "    for collection in vectorstore._client.list_collections():\n",
    "        ids = collection.get()['ids']\n",
    "        print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "        if len(ids): collection.delete(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**FLOOD WARNING** at Inverurie\n",
      "\n",
      "\n",
      "According to the context, there is a **FLOOD WARNING** at Inverurie.\n",
      "[Document(page_content='**FLOOD WARNING** at Inverurie', metadata={'date': '2023-10-19 17:34:15+00:00'}), Document(page_content='Localised Flood Warning issued for Findhorn, Nairn, Moray and Speyside region', metadata={'date': '2023-10-19 12:47:34+00:00'}), Document(page_content='flooding in Forfar Angus getting pretty bad with red warning due to storm babet', metadata={'date': '2023-10-19 17:33:40+00:00'}), Document(page_content=\"The Red warning's been extended to: Aberdeenshire Angus Dundee Perth &amp; Kinross This means extremely dangerous travel conditions and floodwater could cause a danger to life. Please avoid travel in these areas. More safety advice here:\", metadata={'date': '2023-10-19 13:23:57+00:00'}), Document(page_content='Poor Brechin, may be told to evacuate. Hope all are safe. How to help? we need a thoughtful evaluation of all flood risk across Scotland. We need developers &amp; planners to step up to the increased risk', metadata={'date': '2023-10-19 12:03:38+00:00'}), Document(page_content='With weather warnings in force across Scotland, we want to encourage you to follow for detailed flooding updates', metadata={'date': '2023-10-19 15:03:00+00:00'}), Document(page_content='So we introduced a new Emergency Alert system in April that will alert us in life threatening situations like Flooding. Hope the people on the East Coast of Scotland will get theirs if it gets really bad.', metadata={'date': '2023-10-19 14:41:07+00:00'}), Document(page_content='Breaking: Flood warning issued along the River Don in Aberdeenshire', metadata={'date': '2023-10-19 17:59:34+00:00'}), Document(page_content='**FLOOD WARNING** at Blairgowrie to the River Isla', metadata={'date': '2023-10-19 16:02:36+00:00'})]\n"
     ]
    }
   ],
   "source": [
    "#  LLM initialisation\n",
    "model = chatModel_llama3_8B\n",
    "\n",
    "# Load the data from source\n",
    "data = json_dataloader()\n",
    "\n",
    "# Convert to vector store\n",
    "vectorstore = data_embedding(data, eModel = \"bge-large-en-v1.5\", rType = \"Query\")\n",
    "\n",
    "question = \"**FLOOD WARNING** at Inverurie\" #**FLOOD WARNING** at Inverurie\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 4}), llm=llm, include_original=True\n",
    ") #search_kwargs={'k': 40}\n",
    "\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={'k': 40})\n",
    "\n",
    "\n",
    "default_prompt = PromptTemplate(template = prompts.prompt_template_llama3_loc, input_variables = ['question', 'context'])\n",
    "default_chain = RetrievalQA.from_chain_type(llm = model,\n",
    "                        chain_type='stuff',\n",
    "                        retriever=retriever_from_llm,\n",
    "                        chain_type_kwargs={\"prompt\": default_prompt},\n",
    "                        return_source_documents=True\n",
    "                        )\n",
    "\n",
    "ans = default_chain.invoke(question)\n",
    "print(ans['query'])\n",
    "print(ans['result'])\n",
    "print(ans['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
